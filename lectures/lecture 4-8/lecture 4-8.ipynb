{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13417ae-2c28-4815-9bb8-0ba1c797c856",
   "metadata": {},
   "source": [
    "Handling Imbalanced Datasets <b></b><br>\n",
    "An <b> imbalanced </b>dataset has one class under-represented. <br>\n",
    "e.g. Fraudulent e-commerce transactions are much less common than genuine ones. Noise puts <br>\n",
    "genuine ones on the wrong side of the desired decision boundary, moving it to a <b>bad</b> place. <br>\n",
    "<br>\n",
    "Possible solutions: <br>\n",
    " For SVM, we can assign  <b>higher weight</b> to the minority class. <br>\n",
    "e.g. For binary SVM, instead of finding soft-margin’s <br>\n",
    "argminw,b h\n",
    "1\n",
    "2\n",
    "||w||2 + C\n",
    "1\n",
    "N\n",
    "PN\n",
    "i=1 max(0, 1 − yi(wxi + b))i <br>\n",
    "<br>\n",
    "we find something like\n",
    "argminw,b\n",
    "\n",
    "\n",
    "1\n",
    "2\n",
    "||w||2 + C\n",
    "1\n",
    "N\n",
    "\n",
    "C1\n",
    "X\n",
    "{(xi,yi)|yi=−1}\n",
    "max(0, 1 − yi(wxi + b)) +\n",
    "C2\n",
    "X\n",
    "{(xi,yi)|yi=+1}\n",
    "max(0, 1 − yi(wxi + b))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "where C1 and C2 are regularization parameters that can be set as <b>weights</b>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247760c9-ab42-4d60-a508-3093b401772d",
   "metadata": {},
   "source": [
    "<b></b><br>\n",
    "e.g. See Burkov’s Figure 8.1 on p. 98 (p. 3 of www.dropbox.com/s/im1s2skkaikzrrs/Chapter8.pdf?dl=0).<b></b><br>\n",
    "The same problem (before re-weighting imbalanced data) arises with most algorithms.<br>\n",
    " <b>oversampling</b>adds multiple copies of minority class examples.<br>\n",
    " <b>undersampling</b>randomly removes some majority examples (e.g. to save <b>money and time</b>).<br>\n",
    " Create <b>fake</b> examples by combining randomly sampled feature values from several\n",
    "examples of minority class. <br>\n",
    "Do train_test_split() <b>before</b> addressing imbalance so that test data are <b>realistic</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bffe08-f513-4bdb-8960-f3266b959c5e",
   "metadata": {},
   "source": [
    "Combining Models <br> <b></b>\n",
    "While ensemble methods like random forests combine several similar weak models, we can also <br>\n",
    "combine different <b>strong</b> models: <br>\n",
    " <b>average</b> the predictions (regression) or scores (classification) of several models. <br>\n",
    " Majority vote applies several models and returns the  <b>most frequent</b> predicted class. <br>\n",
    "(Resolve a tie by random choice or return an error; or use an odd number of models.) <br>\n",
    "  <b>stacking</b>builds a meta-model whose input is the output of several base models. e.g. To <br>\n",
    "combine models f1 and f2 that predict from the same set of classes, create a training example\n",
    "(x\n",
    "′\n",
    "i\n",
    ", y′\n",
    "i\n",
    ") for the stacked model as (x\n",
    "′\n",
    "i = [f1(xi), f2(xi)], y′\n",
    "i = yi)\n",
    "4 and train a meta-model on\n",
    "the new examples. Tune hyperparameters with cross-validation. Comparatave notes: <br>\n",
    "– Stacking uses  <b>more information</b>from the base models (scores across C class\n",
    "labels) than averaging or majority voting (single best class label from among C labels). <br>\n",
    "– Stacking uses <b>different</b> models on the same data, while bagging uses the\n",
    "model on different (bootstrap resampled) data. <br>\n",
    "– Stacking uses <b>one model</b> to combine predictions from base models, while boosting uses a sequence of models in which the next model tries to correct the current one. <br>\n",
    "Base models should be <b>undercorrelated</b> by being made from different features or different algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039167b-4125-4f6f-86bd-fbb735389af8",
   "metadata": {},
   "source": [
    "Algorithm Efficiency <br>\n",
    "Analysis of algorithms reveals the computational complexity of algorithms in terms of the <br>\n",
    "time (or memory or other resources) they require. We use BIG O notation to write time <br>\n",
    "as a function of input size N, and then ingnore constants and lower-order terms. <br>\n",
    " Suppose a program running on input of size n has run time f(n) seconds. <br>\n",
    " Big-O gives an upper bound on run-time to within a constant factor. A function f(n) is said to\n",
    "be O(g(n)) if there exist constants C and N such that f(n) < C · g(n) for all n > N . <br>\n",
    " Read “f(n) = O(g(n))” as “f(n) is big-O of g(n).” <br>\n",
    " Here are some typical g(n) functions in increasing order: <br>\n",
    "– g(n) = 1, e.g. <b>array lookup </b>by index i <br>\n",
    "– g(n) = log2\n",
    "(n), e.g. <b>binary search</b> in sorted array <br>\n",
    "– g(n) = n, e.g. <b>linear search</b> <br>\n",
    "– g(n) = n log2\n",
    "(n), e.g. clever comparison <b>sorter</b><br>\n",
    "– g(n) = n\n",
    "2\n",
    ", e.g. <b>solution sort</b><br>\n",
    "– g(n) = n\n",
    "3\n",
    ", e.g.<b></b> matrix <b>multiplication</b> , C = AB via cij =\n",
    "Pn <br>\n",
    "k=1 aik · bkj\n",
    "– g(n) = n!, e.g. traveling salesman via <b>brute force</b><br>\n",
    " Just reading a data set of size n is O(n), so an O(n) algorithm (that runs only once)\n",
    "counts as fast. Since log2\n",
    "(n) is small for typical n, an O(n log2\n",
    "(n)) algorithm is often fast enough. Programs taking O(n\n",
    "2\n",
    ") or more time may work for small n but can be too slow\n",
    "for large n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fded28-fd2e-4305-8052-a230e8b15670",
   "metadata": {},
   "source": [
    " The order of the algorithm usually matters a lot more than processor speed, coding\n",
    "skill, programming language, etc. <br>\n",
    " If we cannot figure out the O() formula, we can time the code for several dataset sizes\n",
    "N and make a graph of time vs. N. e.g. <br>\n",
    "start = time.time() # get time in seconds since \"time started\" (often 1/1/1970) <br>\n",
    "]... code that requires timing goes here ... <br>\n",
    "end = time.time() <br>\n",
    "seconds = end - start <br>\n",
    "print(f'The code took {seconds} seconds.') <br>\n",
    " When the time is too long on N examples, work with a randomly-selected subset. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
