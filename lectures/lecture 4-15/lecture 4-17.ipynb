{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3730dae5-73a1-454a-9d73-1d6fa49893f6",
   "metadata": {},
   "source": [
    "For each k ̸= −1 in db.labels_, we can find non-core neighbors in cluster k: <br>\n",
    "is_in_cluster_k = (db.labels_ == k) <br>\n",
    "is_core_sample = np.zeros(shape=db.labels_.shape) <br>\n",
    "is_core_sample[db.core_sample_indices_] = True <br>\n",
    "is_neighbor_of_cluster_k = (is_in_cluster_k & ~is_core_sample) <br>\n",
    " <br>\n",
    "  HDBSCAN improves upon DBSCAN and can handle clusters of varying density. (It\n",
    "drops ϵ. Details are omitted.) Try it first <br>\n",
    "\n",
    "there is a graphical comparison of many clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b964f86-5475-4a86-8b82-56396a00fb90",
   "metadata": {},
   "source": [
    "Dimensionality Reduction\n",
    "Dimensionality reduction maps x into a vector with fewer features to reduce correlation\n",
    "among features, reduce noise, visualize data (we see only 2D or 3D), and facilitate interpretable\n",
    "models. <br>\n",
    " Principal component analysis (PCA) fits a new coordinate system to {xi} where each new\n",
    "is called a principal component (PC):<br>\n",
    "– Each PC is a unit vector (length 1).<br>\n",
    "– The first PC is the direction of the greatest variability  of the data {xi}. (It\n",
    "is the long axis of a “minimal” ellipsoid enclosing the data.)<br>\n",
    "– For i > 1, the ith PC is orthogonal6\n",
    "to the first i − 1 PCs and in the direction of the ith\n",
    "greatest variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc863ad9-cc2e-4a5b-ad08-0ab6ca1996b7",
   "metadata": {},
   "source": [
    "To do dimensionality reduction, we choose some number p of dimensions (0 < p < D) and\n",
    "project each xi onto the first p PCs, transforming the D-dimensional xi\n",
    "into a smaller\n",
    "p-dimensional example. Burkov omits details.\n",
    "Benefits of PCA:\n",
    "– PCA does data compression while retaining most of the information, saving\n",
    "memory, disk space, and computation time.\n",
    "– PCA can mitigate the curse of dimensionality: as D increases, the “volume”\n",
    "of the feature space increases faster than the available data, which become sparse.\n",
    "Many elementary models/algorithms/insights are not designed for sparse data.\n",
    "e.g. The number of D-digit binary numbers in {0, 1}\n",
    "D is 2^d. The number of\n",
    "D-digit decimal numbers in {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    "D is 10^d.\n",
    "e.g. We need N = 10D points to sample each unit interval/square/cube/hypercube from\n",
    "[0, 10]D. Draw [0, 10]D and unit hypercubes for each D ∈ {1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf361f-c77d-4dd8-9d51-1b8365f42107",
   "metadata": {},
   "source": [
    "PCA does Feature Extraction, creates several new features, as linear combinations of original features, not the same as feature selection. <br>\n",
    "the firs 2 or 3 PCs often account for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa650088-8e43-4d43-83e6-33f308ea2f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f7e3168-5239-40af-b0f5-7375c0066a83",
   "metadata": {},
   "source": [
    "PCA is sensitive to feature scaling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
