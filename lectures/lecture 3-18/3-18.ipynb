{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24fcf3a0-a9c7-4d5d-bddd-d484b6d57ccd",
   "metadata": {},
   "source": [
    "cross validation\n",
    "hyper parameters\n",
    "\n",
    "Model performance assessment\n",
    "    MSE = 1/n * sum(x-ti)^2 on training data\n",
    "    overfitting occurs if trainingMSE is smaller than testMSE\n",
    "    yhat = wx + b = 0x + b = ybar\n",
    "    dont report MSE without context of reporting yaverage and ystddev\n",
    "\n",
    "Classification\n",
    "    confusion matricies\n",
    "\n",
    "         predicted ˆy\n",
    "actual y 0                 1\n",
    "0    # TRUE negative (TN)  # false positive (FP)\n",
    "1    # false negative (FN) # TRUEpositive (TP)\n",
    "\n",
    "\n",
    "accuracy\n",
    "    number of correct preditions / number of predicitons\n",
    "    true positives+ true negatives / sum of all four cells\n",
    "\n",
    "precision =\n",
    "#correct positive predictions / #positive predictions = TP / TP + FP\n",
    "the ability to NOT label as positive\n",
    "\n",
    "recall =\n",
    "#correct positive predictions / #positive examples = TP / TP + FN\n",
    "the ability to find all positive examples\n",
    "\n",
    "precision: is proportion of relevant documents in the returned list.\n",
    "recall: is proportion of relevant documents returned to relevant documents available\n",
    "\n",
    "g. In spam (= positive = 1) detection, we want high precision to avoid calling a message\n",
    "spam when it is not (FP). We accept lower recall putting some spam in our inbox (FN).\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "114a5bcb-23a7-4d2f-b57e-57d4b43c3902",
   "metadata": {},
   "source": [
    "We usually must choose between precision and recall, e.g.:\n",
    "– Assign higher weight to examples of a specific class.\n",
    "– Tune hyperparameters to maximize one.\n",
    "– Vary decision threshold, e.g. make a positive prediction only if model probability is\n",
    "  higher than a number larger than 0.5.\n",
    "\n",
    "\n",
    "An ROC curve assesses a classifier that returns a probability along with its prediction.4\n",
    "True positive rate\n",
    "    true positive predicitons/ positive examples = TP / TP + FN = recall\n",
    "false positive rate\n",
    "    false positive prediction / negative eexamples = FP / FP + TN = proportion of negative     examples predicted incoreectly\n",
    "\n",
    "\n",
    "To draw ROC curve,\n",
    "    – use each of several values t ∈ [0, 1] (e.g. from a model) as a prediction probability\n",
    "        threshold t, predict labels, and find TPR and FPR. Note:\n",
    "        * t = 0 =⇒ yˆ = 1 for every x, so (1, 1) is on each ROC curve\n",
    "        * t = 1 =⇒ yˆ = 0 for every x, so (0, 0) is on each ROC curve\n",
    "    – plot resulting {(x =FPR, y =TPR)} pairs5\n",
    "\n",
    "The higher the area under the ROC curve (AUC), the better:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee04add9-1fcf-4028-95b5-aa5846b2e759",
   "metadata": {},
   "source": [
    "– The diagonal line TPR = FPR corresponds to random guessing and has AUC = 1/2\n",
    "because in N trials with P(y = 1) = p, guessing with P(ˆy = 1) = c, we\n",
    "expect this matrix, TPR and FPR:\n",
    "\n",
    "\n",
    "predicted ˆy\n",
    "actual y      0      1 rate\n",
    "0           TN = N(1-p)(1-c) FP = N(1-p)c FPR = c\n",
    "1           FN = Np(1-c) TP = Npc          TPR = c\n",
    "\n",
    "AUC < 0.5 (worse than guessing) indicates a problem\n",
    "AUC = 1 corresponds "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
