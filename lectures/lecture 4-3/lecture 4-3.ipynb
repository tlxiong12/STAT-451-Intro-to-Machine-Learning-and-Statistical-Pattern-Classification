{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf43da8-edb3-4467-940b-d41e2cf980bd",
   "metadata": {},
   "source": [
    "Ensemble Learning <br>\n",
    "    As simple models may perform poorly, in ensemble learning we combine many <b>weak</b>\n",
    "    models (each only a little better than random guessing) to get a strong meta-model.\n",
    "    Ensemble models work because good models agree on the same prediction, while (uncorrelated)\n",
    "    bad models <b>disagree</b> on different ones.\n",
    "    <b>Averaging </b>makes several models independently and averages their predictions to reduce overfitting:\n",
    "<br>\n",
    "<br>\n",
    "Bagging (short for “bootstrap aggregating”) generates B random sets of examples {Si\n",
    ": i =\n",
    "1, . . . , B} of size N by <b>resampling with replacement</b> from the training data.\n",
    "For each Si\n",
    ", train a decision tree fi\n",
    ". Make a prediction:<br> <b></b>\n",
    "– for classification as the most frequent of the B predictions<br>\n",
    "– for regression as the average of the B predictions, ˆy = ˆf(x) = 1\n",
    "B\n",
    "X\n",
    "B\n",
    "i=1\n",
    "fi(x) <br>\n",
    "<br>\n",
    "A random forest is a bagging variant that strives for uncorrelated trees by selecting a random\n",
    "subset of <b>feature</b> for each tree. Hyperparameters include<br>\n",
    "the maximum <b>depth</b> d of each tree<br>\n",
    "– the <b>number</b> B of trees<br>\n",
    "– the number of <b>features</b> to include per subset<br>\n",
    "<br>\n",
    "Boosting builds models <b>sequentially</b> to reduce underfitting:<br>\n",
    "\n",
    "<br>\n",
    "Boosting iteratively creates models such that model (i + 1) is trained to correct model i’s\n",
    "errors by <b>reweighing</b> training examples to increase the weight of mis-classified\n",
    "examples and decrease the weight of correctly-classified examples. The final ensemble model\n",
    "combines all the models.<br>\n",
    "<br>\n",
    "<br>\n",
    "For gradient boosting:\n",
    "– For regression:\n",
    "∗ Start with a constant model f = f0(x) = 1\n",
    "N\n",
    "X\n",
    "N\n",
    "i=1\n",
    "yi = ybar<br>\n",
    ".\n",
    "∗ Calculate <b>residual</b> ei = yi − f(xi) for i = 1, . . . , N.\n",
    "1 Then train a new model\n",
    "f1 with the original y values replaced by the <b>residuals</b> . The boosted model\n",
    "is then f = f0 + αf1, where hyperparameter α is the learning rate.<br>\n",
    "∗ Repeat by training f2 on residuals with respect to f1 and get the boosted model\n",
    "f = f0 + αf1 + αf2, and so on, until making model fM, where M is the maximum\n",
    "number of trees<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d822cbbf-0101-4168-a15a-67834d83a51f",
   "metadata": {},
   "source": [
    "<br><b></b>\n",
    "Model i + 1 is trained to <b>correct the errors</b> of model i. Recall that in gradient\n",
    "descent we move our parameter vector by step size α opposite the direction of the\n",
    "gradient toward the value that minimizes an objective function. Gradient boosting uses <b>residuals</b>\n",
    "as a proxy for the gradient, as they show how to adjust the model to <b>correct errors</b>\n",
    ". Again α limits the amount the model moves in one step.\n",
    "Burkov asserts the overall model f minimizes MSE.<br>\n",
    "<br>\n",
    "Hyperparameters include:<br>\n",
    "∗ the number of trees<br>\n",
    "∗ the learning rate<br>\n",
    "∗ the of trees<br>\n",
    "Boosting reduces (where bagging reduced overfitting); the\n",
    "depth and number of trees can help boosting avoid overfitting.\n",
    "– There is also gradient boosting for classification and for other tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
