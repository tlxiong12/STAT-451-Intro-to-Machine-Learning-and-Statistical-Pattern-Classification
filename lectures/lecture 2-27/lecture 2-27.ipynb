{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df554251-bcb3-4a2b-9f38-edb03d33fe8e",
   "metadata": {},
   "source": [
    "Feature engineering\n",
    "    data cleaning, transforming of data into a set of labeled examples of selected features.\n",
    "\n",
    "\n",
    "One hot encoder\n",
    "    we have categorial labels, we want to convert them, which we use onehotencoder\n",
    "    one hot encoding skipts the trouble of doing it manually and doing it wrong\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c438b8b-31be-4cad-973e-54df3d9d10c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'other' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m pd\u001b[38;5;241m.\u001b[39mget_dummies(df, drop_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# also try drop_first=True\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Join columns from other DataFrame:\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m df\u001b[38;5;241m.\u001b[39mjoin(other)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Do both:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m df\u001b[38;5;241m.\u001b[39mjoin(pd\u001b[38;5;241m.\u001b[39mget_dummies(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn_to_encode\u001b[39m\u001b[38;5;124m'\u001b[39m], drop_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'other' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {'col1': np.random.randint(0, 10, 100), \n",
    "        'col2': np.random.rand(100),\n",
    "        'col3': np.random.choice(['A', 'B', 'C'], 100)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Make one-hot dummy variables from column(s) in DataFrame (or array) data:\n",
    "pd.get_dummies(df, drop_first=False) # also try drop_first=True\n",
    "# Join columns from other DataFrame:\n",
    "df.join(other)\n",
    "# Do both:\n",
    "df.join(pd.get_dummies(df['column_to_encode'], drop_first=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab1920-a1d7-43b4-941e-dcc1f30d3c07",
   "metadata": {},
   "source": [
    "Binning\n",
    "    converts a number feature to categorial\n",
    "    numbers mean a category\n",
    "        ex: 3,5 = baby\n",
    "        this reduces nuber of training examples by reducing the complexity of the model and telling it that values should be treated the same\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e16689-3896-44ae-b649-09b15221c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.cut(x, bins, right=True, labels=None) #puts values from x (array-like) into bins\n",
    "# bins, if bins is a number, or into bins whose edges are in the sequence bins, that (by default)\n",
    "# include the rightmost edge. Use provided labels (if not None).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4edc80-0b6f-465e-a13a-02fb40a11bbd",
   "metadata": {},
   "source": [
    "resacling\n",
    "    improves training speed by preventing large scale features from dominating small scale features. increases performance and prevents overflow and underflow\n",
    "\n",
    "sstandardizato]ion\n",
    "    replaces x wita function, and the features are the mean and standard deviation, and follows normal distribtion, N(0,1)\n",
    "\n",
    "\n",
    "\n",
    "decision trees\n",
    "    scaling doesnt do much? \n",
    "\n",
    "data imputation\n",
    "    remove features with missing values, or replace them with an other value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c4423-7954-4703-accf-e351f81518f5",
   "metadata": {},
   "source": [
    "you can use values outside of a range in order to learn how the model will handle it\n",
    "    if youre missing a value, you can use other features in order to help predict what the    value will be\n",
    "\n",
    "apply data amputation to all training data\n",
    "    otherwise it will be skew\n",
    "\n",
    "fature selection\n",
    "    process of choosing a subset of features for use in a model\n",
    "        improves accuracy\n",
    "            redueces overfiting\n",
    "        improves computing time\n",
    "        makes model easier to interpret\n",
    "\n",
    "\n",
    "feature importance\n",
    "    decrease in model score when model features are randomly shuffled\n",
    "        features could be low importance in one model, but high in another\n",
    "        small groups of features can be favored over large groups\n",
    "        none of a set of colinear features may show importance\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
