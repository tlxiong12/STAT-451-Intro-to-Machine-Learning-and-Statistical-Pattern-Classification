{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "358a6cb1-2eac-4a81-be6b-b51d2448795b",
   "metadata": {},
   "source": [
    "ALgorithm selection\n",
    "     Explainability: If you must be able to explain how predictions are made, consider kNN, linear\n",
    "    regression, and decision tree algorithms. (This may sacrifice performance.)\n",
    "    \n",
    "     Memory: If data do not fit in memory or arrive in an ongoing stream (e.g. stock market\n",
    "    information), consider an incremental learning algorithm.\n",
    "    e.g. A model trained by stochastic gradient descent (§4) can run another iteration to shift\n",
    "    the model toward a new training example.\n",
    "    \n",
    "     Data set size: Neural networks (STAT 453) and gradient boosting (coming in §7) can use\n",
    "    many examples and features. SVM and others cannot handle large data sets as well.\n",
    "    \n",
    "     Categorical vs. numeric features: Choose a suitable algorithm for your feature types, or\n",
    "    convert features to the right type (e.g. one-hot encoding or binning, above).\n",
    "    Nonlinearity:\n",
    "        – If data are linearly separable, consider SVM with the linear kernel.\n",
    "        – If a linear model is suitable, consider linear or logistic regression.\n",
    "        – Otherwise consider a deep neural network or ensemble method (§7)\n",
    "    training speed\n",
    "        cosnider since  neural networks are slow, trees are faster,\n",
    "    prediction speed\n",
    "        SVM regression is fast at prediction, can we understand it?\n",
    "        takes a while with new dataset, but when prediciting a dataset, you just have to solve for wx+b\n",
    "    regression\n",
    "        find wx+b and done, nothing to compare too\n",
    "\n",
    "    KNN\n",
    "        slow, but you need to find its nearest neighbors and you need to go thorugh the whole DF, which is expenive\n",
    "\n",
    "\n",
    "How to choose?????\n",
    "    pick something to optimize, and use a validation set to figure it out\n",
    "\n",
    "\n",
    "use training to set parameters\n",
    "use valudation to choose hyperparameters (knn, use k for nearest neighbors, etc)\n",
    "test data to give data on the outcome we want\n",
    "\n",
    "use 100% of data to set parameters, and choose best hyperparameters\n",
    "    choose different Ks, and ddo whats best outome\n",
    "    try new data, fails because overfitted\n",
    "    all data is unique, evaluation examples, report the y value,\\\n",
    "    overfitting data is bad, dont use 100% of data to set parameters and hyperparameters because it will lead to overfitting\n",
    "\n",
    "footnote:\n",
    "    no one cares about training performance, but Knn k = 1, its about memorizing data, you can find the data, and tell the y value.\n",
    "    we care about the seen data\n",
    "    use test data for the model, do it only once, because it is cheating if you do it more than once \n",
    "    get a df, is expensive, and train on it, validate it and evaluate it, and has an 98% accuracy, and fix the errors. you look and when you see it, it was a mistake, and you fix it. but when you keep going to errors, you can understand, and fix the issues, when you fix them, the errors get smaller, but it is still 9% wrong, and now you are over estimating. Its because you will always get stuck, its tricky, and try to fix it, and disclose any issues.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452767b1-6967-41f8-965e-cf3b5b44e926",
   "metadata": {},
   "source": [
    "train_test_split\n",
    "\n",
    "call it on x and y, and the data is split,\n",
    "    x and y are now multiple arrays, x is now two of them, and so is y\n",
    "    give each split a proportion of samples,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5681b-30ba-46c0-a5c7-e9ce497cddb8",
   "metadata": {},
   "source": [
    "Underfitting and Overfitting\n",
    " A model underfits (or has high bias) when it makes many mistakes on training data because:\n",
    "– the model is too simple for the data\n",
    "– the engineered features are not informative enough\n",
    "model overfits when data is too complex, or it doesnt fit new data, or has too many features, based on the training examples\n",
    "    to reduce overfitting, use a simple modeol, reduce samples of data, add more training data, regualrlize \n",
    "    regularizing\n",
    "        addresses overfitting by inducing a simpler model, often resulting in a bias-variance tradeoff in which bias increases but variance decreases, improving accuracy on unseen examples.\n",
    "\n",
    "you cant use gradient descent on lasso\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
