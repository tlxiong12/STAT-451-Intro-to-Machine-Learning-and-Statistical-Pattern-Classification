{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e154035d-307d-4aef-ae56-e56b5334de56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leoxi\\AppData\\Local\\Temp\\ipykernel_2700\\3749520299.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['True'] = df['label'].replace({'Fake':0, 'Real': 1}) #we make a new column called \"True\" and changing the string \"Fake\" to 0 and \"Real\" to 1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>state</th>\n",
       "      <th>date_published</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_images</th>\n",
       "      <th>has_videos</th>\n",
       "      <th>readability_score</th>\n",
       "      <th>num_shares</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>political_bias</th>\n",
       "      <th>fact_check_rating</th>\n",
       "      <th>is_satirical</th>\n",
       "      <th>trust_score</th>\n",
       "      <th>source_reputation</th>\n",
       "      <th>clickbait_score</th>\n",
       "      <th>plagiarism_score</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>30-11-2021</td>\n",
       "      <td>The Onion</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>1302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.18</td>\n",
       "      <td>47305</td>\n",
       "      <td>450</td>\n",
       "      <td>Center</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>6</td>\n",
       "      <td>0.84</td>\n",
       "      <td>53.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>02-09-2021</td>\n",
       "      <td>The Guardian</td>\n",
       "      <td>Technology</td>\n",
       "      <td>0.92</td>\n",
       "      <td>322</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41.10</td>\n",
       "      <td>39804</td>\n",
       "      <td>530</td>\n",
       "      <td>Left</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>28.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>13-04-2021</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0.25</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.04</td>\n",
       "      <td>45860</td>\n",
       "      <td>763</td>\n",
       "      <td>Center</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>08-03-2020</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0.94</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>75.16</td>\n",
       "      <td>34222</td>\n",
       "      <td>945</td>\n",
       "      <td>Center</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0.92</td>\n",
       "      <td>32.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           state date_published          source       category  \\\n",
       "0   1       Tennessee     30-11-2021       The Onion  Entertainment   \n",
       "1   2       Wisconsin     02-09-2021    The Guardian     Technology   \n",
       "2   3        Missouri     13-04-2021  New York Times         Sports   \n",
       "3   4  North Carolina     08-03-2020             CNN         Sports   \n",
       "\n",
       "   sentiment_score  word_count  has_images  has_videos  readability_score  \\\n",
       "0            -0.22        1302           0           0              66.18   \n",
       "1             0.92         322           1           0              41.10   \n",
       "2             0.25         228           0           1              30.04   \n",
       "3             0.94         155           1           0              75.16   \n",
       "\n",
       "   num_shares  num_comments political_bias fact_check_rating  is_satirical  \\\n",
       "0       47305           450         Center             FALSE             1   \n",
       "1       39804           530           Left             Mixed             1   \n",
       "2       45860           763         Center             Mixed             0   \n",
       "3       34222           945         Center              TRUE             1   \n",
       "\n",
       "   trust_score  source_reputation  clickbait_score  plagiarism_score  True  \n",
       "0           76                  6             0.84             53.35     0  \n",
       "1            1                  5             0.85             28.28     0  \n",
       "2           57                  1             0.72              0.38     0  \n",
       "3           18                 10             0.92             32.20     0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"fake_news_dataset.csv\") #reading the dataset \n",
    "df = df[['id', 'state', 'date_published', 'source',                       \n",
    "       'category', 'sentiment_score', 'word_count', 'has_images',\n",
    "       'has_videos', 'readability_score', 'num_shares', 'num_comments',\n",
    "       'political_bias', 'fact_check_rating', 'is_satirical', 'trust_score',\n",
    "       'source_reputation', 'clickbait_score', 'plagiarism_score', 'label']]   #we dont want author, title, character count or text because the\n",
    "                                                                               #information is irrelevant, so we remove it\n",
    "\n",
    "df.dropna #we drop all NA values\n",
    "\n",
    "df['True'] = df['label'].replace({'Fake':0, 'Real': 1}) #we make a new column called \"True\" and changing the string \"Fake\" to 0 and \"Real\" to 1.\n",
    "\n",
    "df = df[['id', 'state', 'date_published', 'source',\n",
    "       'category', 'sentiment_score', 'word_count', 'has_images',\n",
    "       'has_videos', 'readability_score', 'num_shares', 'num_comments',\n",
    "       'political_bias', 'fact_check_rating', 'is_satirical', 'trust_score',\n",
    "       'source_reputation', 'clickbait_score', 'plagiarism_score', 'True']]   #we do the same thing, we just remove  'label'\n",
    "  \n",
    "df.head(4)  #print out the first four samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca491c-e66f-4268-a98b-8789c7ef0943",
   "metadata": {},
   "source": [
    "<b> Questions we want to answer using this data </b>\n",
    "\n",
    "1) Which state might be populated with the most fake news\n",
    "2) What is the difference between real and fake news and how can the model can detect it?\n",
    "3) Are there common topics that are fake or real among the data?\n",
    "4) What is the relationship of increase in fake news when a political event is occuring?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b22cb9-e270-4b35-ba05-9657c4bbbe48",
   "metadata": {},
   "source": [
    "<b> Variables we will be using </b>\n",
    "\n",
    "If you look at the dataset above, we will be using all of these variables. Here are them in a list: <br>\n",
    "<br>\n",
    "'id' : Number label of the news <br>\n",
    "'state' : where the news was published <br>\n",
    "'date_published' : date when the article was published <br>\n",
    "'source' : where the article was published <br>\n",
    "'category' : type of article <br>\n",
    "'sentiment_score' : emotion score <br>\n",
    "'word_count' : word count of article <br>\n",
    "'has_images' : if the article has images <br>\n",
    "'has_videos' : if the article has vidoes <br>\n",
    "'readability_score' : how easy the text is to read <br>\n",
    "'num_shares' : how many times the article was shared <br>\n",
    "'num_comments' : how many comments were left <br>\n",
    "'political_bias' : if the article was more republican or democratic <br>\n",
    "'fact_check_rating' : if the article was fact checked <br>\n",
    "'is_satirical' : if the article is satire <br>\n",
    "'trust_score' : trustworthiness of the article <br>\n",
    "'source_reputation' : reputability of the source <br>\n",
    "'clickbait_score' : how many people clicked on the article <br>\n",
    "'plagiarism_score' : percentage of  work matches sources in  plagiarism database <br>\n",
    "'True' : if the article is fake or real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad8f57-59ee-42ed-9154-2371c021b810",
   "metadata": {},
   "source": [
    "<b> Methods we will use to answer our questions </b> <br>\n",
    "<br>\n",
    "We will use different modeling methods to find out which one gives us a more precise answer, such as using Logistic Regression, Decision Trees, SVMs, kNN, etc. We also want to use train_test_split in order to train our model on \"new\" data, to ensure there isn't any bias, and it is handling the information well. <br>\n",
    "<br>\n",
    "We will also use model data to capure it's accuracy, precision recall, w coefficients and w intercepts to determine accuracy and desired fitting.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
